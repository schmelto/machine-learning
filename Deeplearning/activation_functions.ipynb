{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "activation-functions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/schmelto/machine-learning/blob/main/Deeplearning/activation_functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3vhSZ8x-1l5X"
      },
      "source": [
        "# Activation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPsgVD8iEITE"
      },
      "source": [
        "## Import the necessary libraries\n",
        "\n",
        "We want to use Tensorflow Version 2.0 and therefore state this specifically.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anAbuY1-dzbZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0176beb2-b1f3-49aa-9b6a-350370ac7271"
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYKdhdArMixR"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9BI2UvBJEVM8"
      },
      "source": [
        "## Load of the MNIST-Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HrkKkPW2OIfB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "a5b5c6c9-6a51-45d2-8eb3-0ba5db4a6670"
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = keras. \\\n",
        "  datasets.mnist.load_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_x_4ihQfEeNP"
      },
      "source": [
        "We remember that the pixel values are not yet in normalized form. So we normalize this first by dividing by the maximum pixel value 255:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6onrRWeHOV8b"
      },
      "source": [
        "train_images = train_images / 255.0\n",
        "test_images = test_images / 255.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQlp8ZldEpPG"
      },
      "source": [
        "* The picture with the handwrited 5 has the label 5.\n",
        "* We want to have the lable as a Vektor [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.] which is in the necessary  for our network. This vektor has now on position 5 (starting at 0) a 1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SbkCCvu8Px1s"
      },
      "source": [
        "total_classes = 10\n",
        "train_vec_labels = keras.utils.to_categorical(train_labels, total_classes)\n",
        "test_vec_labels = keras.utils.to_categorical(test_labels, total_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6TSPdilFE6eA"
      },
      "source": [
        "## Design of the networks\n",
        "\n",
        "Now we have normalized the input data and the labels are available as vectors. So we can finally start building the networks for recognizing the handwritten numbers!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V4gc7lPAE_2A"
      },
      "source": [
        "We want to define a very simple network with 3 layers (input layer, hidden layer and output layer):\n",
        "\n",
        "* We use a keras.layers.Flatten layer as the input layer, which distributes the 28x28 matrices that we receive as inputs to 28x28 = 784 neurons\n",
        "* Next, we use a keras.layers.Dense layer with 128 neurons for the hidden layer\n",
        "* We use a keras.layers.Dense layer with 10 neurons as the output layer, since we want to recognize 10 classes (digits from 0-9)\n",
        "\n",
        "We define the individual networks with the different activation functions so that we can then compare them with one another."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEvwJtk7Oppm"
      },
      "source": [
        "# model_no_activation = keras.Sequential([\n",
        "#     keras.layers.Flatten(input_shape=(28, 28)),\n",
        "#     keras.layers.Dense(128), # , activation='sigmoid'),\n",
        "#     keras.layers.Dense(10), #, activation='sigmoid')\n",
        "# ])\n",
        "\n",
        "model_relu = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='relu'),\n",
        "    keras.layers.Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_linear = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='linear'),\n",
        "    keras.layers.Dense(10, activation='linear')\n",
        "])\n",
        "\n",
        "model_sigmoid = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='sigmoid'),\n",
        "    keras.layers.Dense(10, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model_tanh = keras.Sequential([\n",
        "    keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    keras.layers.Dense(128, activation='tanh'),\n",
        "    keras.layers.Dense(10, activation='tanh')\n",
        "])\n",
        "\n",
        "models = [model_relu, model_linear,\n",
        "          model_sigmoid,model_tanh]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jsruqM_GFF6R"
      },
      "source": [
        "## Compiling the networks\n",
        "\n",
        "After we have defined our networks, we need to *compile* them before we can begin training.\n",
        "\n",
        "In this step we define important parameters for the training phase:\n",
        "- The **Optimizer** is the learning algorithm used in training to improve the network. In the last week we already got to know *Gradient Descent* and its optimization *Stochastic Gradient Descent* (SGD).\n",
        "- The **Loss** is the cost function used. The aim during training is to minimize this.\n",
        "- The **metrics** are the metrics evaluated during training. For all classification problems, we are interested in the \"accuracy\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vEVWjCegFLkB"
      },
      "source": [
        "In this example we use\n",
        "- The *Stochastic Gradient Descent* (`\"sgd\"`) learning algorithm as our optimizer.\n",
        "- The `\"mean_squared_error\"` cost function, which, compared to the normal *Squared Error* cost function, does not calculate the sum but the mean of the errors of the output neurons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjC8UAqUOyZ6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ad646ddf-f4e8-4d50-b20d-9668a25c0b17"
      },
      "source": [
        "[\n",
        "  model.compile(\n",
        "      optimizer='sgd',\n",
        "      loss='mean_squared_error',\n",
        "      metrics=['accuracy']\n",
        "  ) for model in models\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[None, None, None, None]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoZmTqF8FSpa"
      },
      "source": [
        "## Training the networks\n",
        "\n",
        "Now we can finally train our network. For this we use the `fit` method and transfer our training images as inputs with the associated labels as desired outputs. The number of `epochs` indicates how often the network gets to see the entire training set. If we increase the number of epochs, we let our network learn longer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QDK4o68O71J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "60e5e310-5fa5-40a7-d409-9283491191ee"
      },
      "source": [
        "epochs=15\n",
        "[\n",
        " model.fit(\n",
        "    train_images,\n",
        "    train_vec_labels,\n",
        "    epochs=epochs,\n",
        "    verbose=True\n",
        "  ) for model in models\n",
        "]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 6s 95us/sample - loss: 0.1064 - accuracy: 0.2881\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0787 - accuracy: 0.4845\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0680 - accuracy: 0.5940\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0587 - accuracy: 0.6867\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 4s 67us/sample - loss: 0.0516 - accuracy: 0.7606\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 4s 68us/sample - loss: 0.0462 - accuracy: 0.7996\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 4s 69us/sample - loss: 0.0421 - accuracy: 0.8213\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0390 - accuracy: 0.8360\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0365 - accuracy: 0.8467\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0345 - accuracy: 0.8525\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0328 - accuracy: 0.8582\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0314 - accuracy: 0.8626\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0302 - accuracy: 0.8661\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0292 - accuracy: 0.8688\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0283 - accuracy: 0.8716\n",
            "Train on 60000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0831 - accuracy: 0.6396\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0536 - accuracy: 0.7952\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0487 - accuracy: 0.8187\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0464 - accuracy: 0.8295\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0449 - accuracy: 0.8354\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0439 - accuracy: 0.8384\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0432 - accuracy: 0.8412\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0426 - accuracy: 0.8430\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0421 - accuracy: 0.8450\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0417 - accuracy: 0.8466\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0414 - accuracy: 0.8461\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0412 - accuracy: 0.8474\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0409 - accuracy: 0.8483\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0407 - accuracy: 0.8493\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0406 - accuracy: 0.8489\n",
            "Train on 60000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.1007 - accuracy: 0.1782\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0884 - accuracy: 0.3011\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0874 - accuracy: 0.3739\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0863 - accuracy: 0.4248\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0850 - accuracy: 0.4713\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0836 - accuracy: 0.5073\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0821 - accuracy: 0.5376\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0803 - accuracy: 0.5520\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0784 - accuracy: 0.5696\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0764 - accuracy: 0.5831\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0743 - accuracy: 0.5949\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0722 - accuracy: 0.6076\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0701 - accuracy: 0.6204\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 4s 63us/sample - loss: 0.0680 - accuracy: 0.6350\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 4s 62us/sample - loss: 0.0661 - accuracy: 0.6468\n",
            "Train on 60000 samples\n",
            "Epoch 1/15\n",
            "60000/60000 [==============================] - 4s 64us/sample - loss: 0.0811 - accuracy: 0.6345\n",
            "Epoch 2/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0545 - accuracy: 0.7996\n",
            "Epoch 3/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0491 - accuracy: 0.8282\n",
            "Epoch 4/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0458 - accuracy: 0.8442\n",
            "Epoch 5/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0434 - accuracy: 0.8560\n",
            "Epoch 6/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0414 - accuracy: 0.8651\n",
            "Epoch 7/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0397 - accuracy: 0.8721\n",
            "Epoch 8/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0382 - accuracy: 0.8778\n",
            "Epoch 9/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0368 - accuracy: 0.8830\n",
            "Epoch 10/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0356 - accuracy: 0.8877\n",
            "Epoch 11/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0345 - accuracy: 0.8909\n",
            "Epoch 12/15\n",
            "60000/60000 [==============================] - 4s 60us/sample - loss: 0.0335 - accuracy: 0.8943\n",
            "Epoch 13/15\n",
            "60000/60000 [==============================] - 4s 59us/sample - loss: 0.0326 - accuracy: 0.8979\n",
            "Epoch 14/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0317 - accuracy: 0.9011\n",
            "Epoch 15/15\n",
            "60000/60000 [==============================] - 4s 61us/sample - loss: 0.0309 - accuracy: 0.9040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<tensorflow.python.keras.callbacks.History at 0x7f74304cfe80>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f74303c8a58>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f74303017b8>,\n",
              " <tensorflow.python.keras.callbacks.History at 0x7f743023f198>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sui4vaiHFvzd"
      },
      "source": [
        "## Evaluate the networks\n",
        "\n",
        "So far, the network has only seen training images and learned from them. The aim is to use our networks to recognize new images of handwritten numbers. For this purpose, there is the test data with which we now want to check our networks for accuracy in the case of unseen data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeujdy_fU0rl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "outputId": "b88486b1-cfdd-4605-e6fd-5b5dfe3e0966"
      },
      "source": [
        "_, result_relu = model_relu.evaluate(test_images, test_vec_labels)\n",
        "_, result_linear = model_linear.evaluate(test_images, test_vec_labels)\n",
        "_, result_sigmoid = model_sigmoid.evaluate(test_images, test_vec_labels)\n",
        "_, result_tanh = model_tanh.evaluate(test_images, test_vec_labels)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0269 - accuracy: 0.8820\n",
            "10000/10000 [==============================] - 1s 58us/sample - loss: 0.0400 - accuracy: 0.8577\n",
            "10000/10000 [==============================] - 1s 61us/sample - loss: 0.0647 - accuracy: 0.6573\n",
            "10000/10000 [==============================] - 1s 59us/sample - loss: 0.0302 - accuracy: 0.9050\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eFHs3Qw3FeS0"
      },
      "source": [
        "## Clear presentation of the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hocTWrYaWBQp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 155
        },
        "outputId": "bf6561fa-64c0-495c-e921-fa9a6a269c85"
      },
      "source": [
        "from prettytable import PrettyTable\n",
        "tbl = PrettyTable()\n",
        "tbl.field_names = [\"Activation function\", f\"Accurracy (after {epochs} epochs)\"]\n",
        "tbl.add_row([\"Tanh\", result_tanh])\n",
        "tbl.add_row([\"model_relu\", result_relu])\n",
        "tbl.add_row([\"Linear\", result_linear])\n",
        "tbl.add_row([\"Sigmoid\", result_sigmoid])\n",
        "print(tbl)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------------------+-----------------------------+\n",
            "| Activation function | Accurracy (after 15 epochs) |\n",
            "+---------------------+-----------------------------+\n",
            "|         Tanh        |            0.905            |\n",
            "|      model_relu     |            0.882            |\n",
            "|        Linear       |            0.8577           |\n",
            "|       Sigmoid       |            0.6573           |\n",
            "+---------------------+-----------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}